{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of From PyTorch to JAX: towards neural net frameworks that purify stateful code",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlmacedo/deep-learning-class/blob/master/Copy_of_From_PyTorch_to_JAX_towards_neural_net_frameworks_that_purify_stateful_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ChZqguta0_E",
        "colab_type": "text"
      },
      "source": [
        "*This post is also available rendered as a blogpost on https://sjmielke.com/jax-purify.htm ! :)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlcaSB3w3nG-",
        "colab_type": "code",
        "outputId": "ad69f476-a2d1-4006-c4e3-be8498acab89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "!echo \"This whole notebook runs in under a minute, half of which is taken up by this package installing... (:\"\n",
        "!pip install --upgrade -q jax jaxlib datascience albumentations coveralls git+https://github.com/deepmind/dm-haiku\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This whole notebook runs in under a minute, half of which is taken up by this package installing... (:\n",
            "\u001b[K     |████████████████████████████████| 276kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 29.3MB 138kB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 44.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.2MB 228kB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 35.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 11.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 48.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 634kB 45.1MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for datascience (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-gKKKxz4pTi",
        "colab_type": "text"
      },
      "source": [
        "# From PyTorch to JAX: towards neural net frameworks that purify stateful code\n",
        "### *Sabrina J. Mielke, 2020-03-09*\n",
        "\n",
        "[JAX](https://github.com/google/jax), Google's now-over-a-year-old Python library for machine learning and other numerical computing describes itself as “Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more”—and while that definition is certainly fitting, it is a bit intimidating. I would describe JAX as numpy, but on GPU, and then move on to the one feature we will be most concerned with in this post: its autodifferentiation capability, i.e., how to get gradients of some loss function your code computes with respect to you input parameters. If you haven't heard of JAX at all, I can recommend [Skye Wanderman-Milne's talk at NeurIPS]( https://slideslive.com/38923687/jax-accelerated-machinelearning-research-via-composable-function-transformations-in-python) on JAX (or check out [the corresponding slides](https://program-transformations.github.io/slides/NeurIPS_workshop_JAX_talk.pdf)). It's a cool framework with cool ideas!\n",
        "\n",
        "That said, moving from PyTorch or Tensorflow 2 to JAX is a huge change: the fundamental way we build up computation and, more importantly, backpropagate through it is fundamentally different in the two! PyTorch builds up a graph as you compute the forward pass, and one call to `backward()` on some “result” node then augments each intermediate node in the graph with the gradient of the result node with respect to that intermediate node. JAX on the other hand makes you express your computation as a Python function, and by transforming it with `grad()` gives you a gradient function that you can evaluate like your computation function—but instead of the output it gives you the gradient of the output with respect to (by default) the first parameter that your function took as input:\n",
        "\n",
        "[![PyTorch vs. JAX on a very simple 1D linear “layer”](https://sjmielke.com/images/blog/jax-purify/comparison_small.png)](https://sjmielke.com/images/blog/jax-purify/comparison_big.png)\n",
        "\n",
        "This has consequences for how you write code and build up models in both frameworks, of course. So when you're used to tape-based auto-differentiation and working with stateful objects in PyTorch or Tensorflow 2, coming to JAX may be quite a shock—and while running `grad()` on numpy-oneliners like the one above (which we will actually run later below) is cool and all, you wonder what a minimal example for, say, a language model would look like (language models aren't quite as straightforward to implement as ResNets: they can have dynamic structures that aren't always nicely divisible into “layers”).\n",
        "\n",
        "Maybe you decided to look at libraries like `flax`, `trax`, or `haiku` and what you see at least in the ResNet examples looks not too dissimilar from any other framework: define some layers, run some trainers... but what is it that actually happens there? What's the route from these tiny numpy functions to training big hierarchical neural nets?\n",
        "\n",
        "That's the niche this post is trying to fill. We will:\n",
        "1. quickly recap a stateful LSTM-LM implementation in a tape-based gradient framework, specifically PyTorch,\n",
        "2. see how PyTorch-style coding relies on mutating state, learn about mutation-free *pure* functions and build (pure) zappy one-liners in JAX,\n",
        "3. step-by-step go from individual parameters to medium-size modules by registering them as pytree nodes,\n",
        "4. combat growing pains by building fancy scaffolding, and controlling context to extract initialized parameters purify functions and\n",
        "5. realize that we could get that easily in a framework like DeepMind's `haiku` using its `transform` mechanism.\n",
        "\n",
        "Things we will *not* do in this tutorial: build state-of-the-art models or elegant, “idiomatic” codebases—we just try to build minimal examples that exhibit the complexity we are looking for. We also won't cover batching (which is easy with `vmap()`), distributing (`pmap()`), and XLA-compiling your code (`jit()`) in JAX—all of which are really cool features, but beside the point.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLiq3sTr8KgG",
        "colab_type": "text"
      },
      "source": [
        "## 1. A LSTM-LM in PyTorch\n",
        "\n",
        "To make sure we're on the same page, let's implement the language model I want to work towards in PyTorch. To keep the comparison straightforward, we will implement things from scratch as much as possible in all three approaches. Let's start with an LSTMCell that holds some parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91cl3Ucp7Rf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class LSTMCell(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.weight_ih = torch.nn.Parameter(torch.rand(4*out_dim, in_dim))\n",
        "        self.weight_hh = torch.nn.Parameter(torch.rand(4*out_dim, out_dim))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(4*out_dim,))\n",
        "        \n",
        "    def forward(self, inputs, h, c):\n",
        "        ifgo = self.weight_ih @ inputs + self.weight_hh @ h + self.bias\n",
        "        i, f, g, o = torch.chunk(ifgo, 4)\n",
        "        i = torch.sigmoid(i)\n",
        "        f = torch.sigmoid(f)\n",
        "        g = torch.tanh(g)\n",
        "        o = torch.sigmoid(o)\n",
        "        new_c = f * c + i * g\n",
        "        new_h = o * torch.tanh(new_c)\n",
        "        return (new_h, new_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4nBTueb-L2x",
        "colab_type": "text"
      },
      "source": [
        "Next, build a super simple 1-layer LSTM language model using this cell. Note that to keep the example simple, we will just use a simple matrix for embeddings. This and the learned $(h,c)_0$ will demonstrate how individual paramaters are registered in our solutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCeaUYaP_n5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMLM(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, dim=17):\n",
        "        super().__init__()\n",
        "        self.cell = LSTMCell(dim, dim)\n",
        "        self.embeddings = torch.nn.Parameter(torch.rand(vocab_size, dim))\n",
        "        self.c_0 = torch.nn.Parameter(torch.zeros(dim))\n",
        "    \n",
        "    @property\n",
        "    def hc_0(self):\n",
        "        return (torch.tanh(self.c_0), self.c_0)\n",
        "\n",
        "    def forward(self, seq, hc):\n",
        "        loss = torch.tensor(0.)\n",
        "        for idx in seq:\n",
        "            loss -= torch.log_softmax(self.embeddings @ hc[0], dim=-1)[idx]\n",
        "            hc = self.cell(self.embeddings[idx,:], *hc)\n",
        "        return loss, hc\n",
        "    \n",
        "    def greedy_argmax(self, hc, length=6):\n",
        "        with torch.no_grad():\n",
        "            idxs = []\n",
        "            for i in range(length):\n",
        "                idx = torch.argmax(self.embeddings @ hc[0])\n",
        "                idxs.append(idx.item())\n",
        "                hc = self.cell(self.embeddings[idx,:], *hc)\n",
        "        return idxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8z9Fx9VBUG8",
        "colab_type": "text"
      },
      "source": [
        "To demonstrate that it works, let's train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpxKj_tuBWf3",
        "colab_type": "code",
        "outputId": "448b96ce-07a5-4327-d7a5-d179d1e57d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# As training data, we will have indices of words/wordpieces/characters,\n",
        "# we just assume they are tokenized and integerized (toy example obviously).\n",
        "\n",
        "import jax.numpy as jnp\n",
        "vocab_size = 43  # prime trick! :)\n",
        "training_data = jnp.array([4, 8, 15, 16, 23, 42])\n",
        "\n",
        "lm = LSTMLM(vocab_size=vocab_size)\n",
        "print(\"Sample before:\", lm.greedy_argmax(lm.hc_0))\n",
        "\n",
        "bptt_length = 3  # to illustrate hc.detach-ing\n",
        "\n",
        "for epoch in range(101):\n",
        "    hc = lm.hc_0\n",
        "    totalloss = 0.\n",
        "    for start in range(0, len(training_data), bptt_length):\n",
        "        batch = training_data[start:start+bptt_length]\n",
        "        loss, (h, c) = lm(batch, hc)\n",
        "        hc = (h.detach(), c.detach())\n",
        "        if epoch % 50 == 0:\n",
        "            totalloss += loss.item()\n",
        "        loss.backward()\n",
        "        for name, param in lm.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                param.data -= 0.1 * param.grad\n",
        "                del param.grad\n",
        "    if totalloss:\n",
        "        print(\"Loss:\", totalloss)\n",
        "\n",
        "print(\"Sample after:\", lm.greedy_argmax(lm.hc_0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:122: UserWarning: No GPU/TPU found, falling back to CPU.\n",
            "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sample before: [42, 34, 34, 34, 34, 34]\n",
            "Loss: 25.953862190246582\n",
            "Loss: 3.7642268538475037\n",
            "Loss: 1.9537211656570435\n",
            "Sample after: [4, 8, 15, 16, 23, 42]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqlHT1FNKXyG",
        "colab_type": "text"
      },
      "source": [
        "So, this is all nice and perhaps more intuitive than old graph-mode Theano or Tensorflow, but it comes with some annoying gotchas. Even while writing this up, I struggled with making sure I `detach`-ed in the right places and marked the right things as `Parameter`s to make sure just the right amount of nodes in the computation graph (which PyTorch assembles through our statements in the background) are marked as intermediate and cleaned up at the right time.\n",
        "\n",
        "\n",
        "## 2. Pure functions\n",
        "\n",
        "To understand how JAX handles this issue, we need to understand the concept of *pure* functions. If you've done some functional programming before, you might be familiar with that concept: a pure function is like a function or formula in math. It defines how an output value is obtained from some input values. What's important is that it has no “side effects”: no part of the function should access or even mutate any global *state*.\n",
        "\n",
        "The way we wrote our code in PyTorch was very much stateful and full of mutating state, making reasoning about it and optimizing it a bit tricky. JAX therefore chooses to constrain a programmer to *pure* functions that don't do any of that.\n",
        "\n",
        "But let's look at some examples of pure functions before we dive into JAX. To confirm that a function is pure, in a nutshell, these criteria have to hold:\n",
        "1. It shouldn't matter when and in what context you execute the function—as long as the inputs are the same, the outputs should be the same.\n",
        "2. Whether we executed the function zero, one, or many times should be absolutely impossible to discern after the fact.\n",
        "\n",
        "See how all the impure functions below violate at least one of these constraints:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK6f8BfWcKUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "nr_executions = 0\n",
        "\n",
        "def pure_fn_1(x):\n",
        "    return 2 * x\n",
        "\n",
        "def pure_fn_2(xs):\n",
        "    ys = []\n",
        "    for x in xs:\n",
        "        # Mutating stateful variables *inside* the function is fine!\n",
        "        ys.append(2 * x)\n",
        "    return ys\n",
        "\n",
        "def impure_fn_1(xs):\n",
        "    # Mutating arguments has lasting consequences outside the function! :(\n",
        "    xs.append(sum(xs))\n",
        "    return xs\n",
        "\n",
        "def impure_fn_2(x):\n",
        "    # Very obviously mutating global state is bad...\n",
        "    global nr_executions\n",
        "    nr_executions += 1\n",
        "    return 2 * x\n",
        "\n",
        "def impure_fn_3(x):\n",
        "    # ...but just accessing it is, too, because now the function depends on the\n",
        "    # execution context!\n",
        "    return nr_executions * x\n",
        "\n",
        "def impure_fn_4(x):\n",
        "    # Things like IO are classic examples of impurity.\n",
        "    # All three of the following lines are violations of purity:\n",
        "    print(\"Hello!\")\n",
        "    user_input = input()\n",
        "    execution_time = time.time()\n",
        "    return 2 * x\n",
        "\n",
        "def impure_fn_5(x):\n",
        "    # Which constraint does this violate? Both, actually! You access the current\n",
        "    # state of randomness *and* advance the number generator!\n",
        "    p = random.random()\n",
        "    return p * x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hEgWV9YcJ4p",
        "colab_type": "text"
      },
      "source": [
        "Let's see a pure function that JAX operates on: the example from the intro figure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDvFtSSuKcWF",
        "colab_type": "code",
        "outputId": "5aad9e91-635a-4f39-b135-985f694e28b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# (almost) 1-D linear regression\n",
        "def f(w, x):\n",
        "    return w * x\n",
        "\n",
        "print(f(13., 42.))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "546.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxcoCKBmehox",
        "colab_type": "text"
      },
      "source": [
        "So far, so uneventful. What JAX now allows you to do is to take a function like this and transform it into a function that instead of returning the result, returns the *gradient of the result* with respect to (by default) the first parameter!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBiWZ60DeiA-",
        "colab_type": "code",
        "outputId": "c9073c7c-273b-43f1-e625-48185c7c34f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Gradient: with respect to weights! JAX uses the first argument by default.\n",
        "df_dw = jax.grad(f)\n",
        "\n",
        "def manual_df_dw(w, x):\n",
        "    return x\n",
        "\n",
        "assert df_dw(13., 42.) == manual_df_dw(13., 42.)\n",
        "\n",
        "print(df_dw(13., 42.))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHbuinl2Xm7T",
        "colab_type": "text"
      },
      "source": [
        "Everything up to here you probably have seen in the [JAX README](https://github.com/google/jax/blob/master/README.md) and it kinda makes sense. But how do we get from here to big modules like the one in our PyTorch code?\n",
        "\n",
        "First, let's add a bias term and try to wrap the 1-D linear regressor we get into an object like we're used to: a kind of `LinearRegressor` “layer“:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJfAsTyLXlXw",
        "colab_type": "code",
        "outputId": "f25066ec-01d6-41a6-864e-b02831a5a2bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "class LinearRegressor():\n",
        "    def __init__(self, w, b):\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.w * x + self.b\n",
        "\n",
        "    def rms(self, xs: jnp.ndarray, ys: jnp.ndarray):\n",
        "        return jnp.sqrt(jnp.sum(jnp.square(self.w * xs + self.b - ys)))\n",
        "\n",
        "my_regressor = LinearRegressor(13., 0.)\n",
        "\n",
        "# A kind of loss fuction, used for training\n",
        "xs = jnp.array([42.0])\n",
        "ys = jnp.array([500.0])\n",
        "print(my_regressor.rms(xs, ys))\n",
        "\n",
        "# Prediction for test data\n",
        "print(my_regressor.predict(42.))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.0\n",
            "546.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylFs7hW3YJQs",
        "colab_type": "text"
      },
      "source": [
        "So far, so good. So how do we get gradients to train? We would need a pure function that has our parameters as arguments somewhere, maybe like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPWvQG6LbTEs",
        "colab_type": "code",
        "outputId": "992d8e70-9f9b-4191-bb91-e6227865128d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def loss_fn(w, b, xs, ys):\n",
        "    my_regressor = LinearRegressor(w, b)\n",
        "    return my_regressor.rms(xs=xs, ys=ys)\n",
        "\n",
        "# We use argnums=(0, 1) to tell JAX to give us\n",
        "# gradients wrt first and second parameter.\n",
        "grad_fn = jax.grad(loss_fn, argnums=(0, 1))\n",
        "\n",
        "print(loss_fn(13., 0., xs, ys))\n",
        "print(grad_fn(13., 0., xs, ys))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.0\n",
            "(DeviceArray(42., dtype=float32), DeviceArray(1., dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSLysMtTcSG4",
        "colab_type": "text"
      },
      "source": [
        "Convince yourself that that's true :)\n",
        "Now, this is workable, but clearly enumerating all parameters in the head of `loss_fn` isn't feasible.\n",
        "\n",
        "Luckily, JAX is not just comfortable differentiating with respect to scalars, vectors, and matrices, but also with respect to a number of *tree-like* data structures that it calls *pytrees*—and they include python dicts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFq5nYejYLmL",
        "colab_type": "code",
        "outputId": "ac183d74-7bfb-46ad-880c-36a0dc7f9be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def loss_fn(params, xs, ys):\n",
        "    my_regressor = LinearRegressor(params['w'], params['b'])\n",
        "    return my_regressor.rms(xs=xs, ys=ys)\n",
        "\n",
        "grad_fn = jax.grad(loss_fn)\n",
        "\n",
        "print(loss_fn({'w': 13., 'b': 0.}, xs, ys))\n",
        "print(grad_fn({'w': 13., 'b': 0.}, xs, ys))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.0\n",
            "{'b': DeviceArray(1., dtype=float32), 'w': DeviceArray(42., dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcj30yIjcySd",
        "colab_type": "text"
      },
      "source": [
        "So this already looks nicer! We could write a training loop like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRIvb2xMdch9",
        "colab_type": "code",
        "outputId": "816b22e8-8986-4dce-9e59-2b784368f066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "params = {'w': 13., 'b': 0.}\n",
        "\n",
        "for _ in range(15):\n",
        "    print(loss_fn(params, xs, ys))\n",
        "    grads = grad_fn(params, xs, ys)\n",
        "    for name in params.keys():\n",
        "        params[name] -= 0.002 * grads[name]\n",
        "\n",
        "# Now, predict:\n",
        "LinearRegressor(params['w'], params['b']).predict(42.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.0\n",
            "42.47003\n",
            "38.940002\n",
            "35.410034\n",
            "31.880066\n",
            "28.350098\n",
            "24.820068\n",
            "21.2901\n",
            "17.760132\n",
            "14.230164\n",
            "10.700165\n",
            "7.170166\n",
            "3.6401978\n",
            "0.110198975\n",
            "3.4197998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(500.1102, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4wrzmnZgr4K",
        "colab_type": "text"
      },
      "source": [
        "Note that we can already make use of a bit more JAX helpers for the updating itself: since params and grads have the same (tree-like) structure, we can imagine laying them on top and creating a new tree whose values everywhere are a “combination” of the two trees like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_O__ACag543",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_combiner(param, grad, lr=0.002):\n",
        "    return param - lr * grad\n",
        "\n",
        "params = jax.tree_multimap(update_combiner, params, grads)\n",
        "# instead of:\n",
        "# for name in params.keys():\n",
        "#    params[name] -= 0.1 * grads[name]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C7aodWfJsj",
        "colab_type": "text"
      },
      "source": [
        "## 3. A lacking impromptu solution: registering classes as custom pytree types\n",
        "\n",
        "So, it works. But going back and forth between our object and the `params` dict is a bit annoying. One thing we can do to simplify the process is allow JAX to see our so-far rather opaque `LinearRegressor` class as a data structure, allowing it to be used in place of the dict `params` we have!\n",
        "\n",
        "For this we will need to tell JAX how one can break our class down into a list of parameters and auxiliary information (called *flattening*) and how then, it can reassemble the class with perhaps changed parameters and that auxiliary information (called *unflattening*):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roFdQ1nxfpoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten_linear_regressor(regressor):\n",
        "    leaves = (regressor.w, regressor.b)\n",
        "    aux = None  # we don't need auxiliary information for this simple class\n",
        "    return (leaves, aux)\n",
        "\n",
        "# careful, switched argument order! (unfortunate baggage from the past...)\n",
        "def unflatten_linear_regressor(_aux, leaves):\n",
        "    w, b = leaves\n",
        "    return LinearRegressor(w, b)\n",
        "\n",
        "jax.tree_util.register_pytree_node(\n",
        "    LinearRegressor,\n",
        "    flatten_linear_regressor,\n",
        "    unflatten_linear_regressor,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwmGlchfgXgC",
        "colab_type": "text"
      },
      "source": [
        "Now we can use our regressor throughout, making for a very easy `loss_fn`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NljW7qZ5gbFa",
        "colab_type": "code",
        "outputId": "9d8350fb-8f20-49fb-f3a1-5f91e8858403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def loss_fn(regressor, xs, ys):\n",
        "    return regressor.rms(xs=xs, ys=ys)\n",
        "\n",
        "grad_fn = jax.grad(loss_fn)\n",
        "\n",
        "print(loss_fn(LinearRegressor(w=13., b=0.), xs, ys))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qiAq1nSiFrR",
        "colab_type": "text"
      },
      "source": [
        "Now what do you think the function `grad_fn` returns? It used to be a dict of gradients in the shape of the `params` dict, but now..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wwqsx2fiLwn",
        "colab_type": "code",
        "outputId": "743182aa-dbed-4fde-d2d7-2d444b372ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(grad_fn(LinearRegressor(w=13., b=0.), xs, ys))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.LinearRegressor object at 0x7f5a9fb91c50>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0f1XlYJiPI5",
        "colab_type": "text"
      },
      "source": [
        "...it is a `LinearRegressor` object that has gradients where the params used to be! Again, we can use `jax.tree_util.tree_multimap` to combine:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M0xIcTCiYKd",
        "colab_type": "code",
        "outputId": "af3b2adb-b7a3-41e4-ab7d-12b7f472cb79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "model = LinearRegressor(w=13., b=0.)\n",
        "\n",
        "for _ in range(15):\n",
        "    print(loss_fn(model, xs, ys))\n",
        "    grads = grad_fn(model, xs, ys)\n",
        "    model = jax.tree_multimap(update_combiner, model, grads)\n",
        "\n",
        "# Now, predict:\n",
        "model.predict(42.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.0\n",
            "42.47003\n",
            "38.940002\n",
            "35.410034\n",
            "31.880066\n",
            "28.350098\n",
            "24.820068\n",
            "21.2901\n",
            "17.760132\n",
            "14.230164\n",
            "10.700165\n",
            "7.170166\n",
            "3.6401978\n",
            "0.110198975\n",
            "3.4197998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(500.1102, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGiDGrM0LUab",
        "colab_type": "text"
      },
      "source": [
        "So trying to make your modules and your models all `flatten`- and `unflatten`-able is one way you can avoid param dict handling (you're essentially writing it once for your `flatten` and `unflatten` and then let JAX call them).\n",
        "\n",
        "The downside is that this requires you to write these flattening/unflattening functions for every module (though much of that can certainly be re-used in a base class like `nn.Module`). Even for small things like our LSTM-LM from above this solution looks plenty ugly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcAtrULLqe1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PytreeLSTMCell():\n",
        "    def __init__(self, weight_ih, weight_hh, bias):\n",
        "        self.weight_ih = weight_ih\n",
        "        self.weight_hh = weight_hh\n",
        "        self.bias = bias\n",
        "\n",
        "    def __call__(self, inputs, h, c):\n",
        "        ifgo = self.weight_ih @ inputs + self.weight_hh @ h + self.bias\n",
        "        i, f, g, o = jnp.split(ifgo, indices_or_sections=4, axis=-1)\n",
        "        i = jax.nn.sigmoid(i)\n",
        "        f = jax.nn.sigmoid(f)\n",
        "        g = jnp.tanh(g)\n",
        "        o = jax.nn.sigmoid(o)\n",
        "        new_c = f * c + i * g\n",
        "        new_h = o * jnp.tanh(new_c)\n",
        "        return (new_h, new_c)\n",
        "\n",
        "jax.tree_util.register_pytree_node(\n",
        "    PytreeLSTMCell,\n",
        "    lambda c: ((c.weight_ih, c.weight_hh, c.bias), None),\n",
        "    lambda _, ws: PytreeLSTMCell(*ws),\n",
        ")\n",
        "\n",
        "class PytreeLSTMLM():\n",
        "    def __init__(self, cell, embeddings, c_0):\n",
        "        self.cell = cell\n",
        "        self.embeddings = embeddings\n",
        "        self.c_0 = c_0\n",
        "    \n",
        "    @property\n",
        "    def hc_0(self):\n",
        "        return (jnp.tanh(self.c_0), self.c_0)\n",
        "\n",
        "    @jax.jit  # jit compiles with XLA, so we are a lot faster (try it without!).\n",
        "    def forward(self, seq, hc):\n",
        "        loss = 0.\n",
        "        for idx in seq:\n",
        "            loss -= jax.nn.log_softmax(self.embeddings @ hc[0])[idx]\n",
        "            hc = self.cell(self.embeddings[idx,:], *hc)\n",
        "        return loss, hc\n",
        "\n",
        "    def greedy_argmax(self, hc, length=6):\n",
        "        idxs = []\n",
        "        for i in range(length):\n",
        "            idx = jnp.argmax(self.embeddings @ hc[0])\n",
        "            idxs.append(int(idx))\n",
        "            hc = self.cell(self.embeddings[idx,:], *hc)\n",
        "        return idxs\n",
        "\n",
        "# These two functions are just a whole lot of unreadable YIKES\n",
        "def flatten_whole_lstmlm(lm):\n",
        "    flat_cell_weights, flat_cell_aux = jax.tree_util.tree_flatten(lm.cell)\n",
        "    return tuple(flat_cell_weights) + (lm.embeddings, lm.c_0), flat_cell_aux\n",
        "\n",
        "def unflatten_whole_lstmlm(aux, weights):\n",
        "    flat_cell_weights = weights[:-2]\n",
        "    embeddings, c_0 = weights[-2:]\n",
        "    cell = jax.tree_util.tree_unflatten(aux, flat_cell_weights)\n",
        "    return PytreeLSTMLM(cell, embeddings, c_0)\n",
        "\n",
        "jax.tree_util.register_pytree_node(\n",
        "    PytreeLSTMLM,\n",
        "    flatten_whole_lstmlm,\n",
        "    unflatten_whole_lstmlm,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo38bfarvXn8",
        "colab_type": "text"
      },
      "source": [
        "Look at that. Nasty.\n",
        "*(Apart from the horrors in the handler, you might've noticed that I added the `@jax.jit` annotation to the `forward` method just to make it run a bit faster—this, like `grad()` only works cause we made our class transparent to JAX—but it doesn't affect functionality, as you can see when you try to remove it. We won't talk about it more in this tutorial.)*\n",
        "\n",
        "But just to show you that it works, let's train it like we trained the PyTorch model. To do that, we will need initial parameters, and that brings us to another initially frightening aspect of JAX: you always have to specify where your randomness comes from!\n",
        "\n",
        "Remember that we said that randomness is also a source of impurity. But, really, in a computer, you never get real randomness, you always get *pseudo-randomness*: random looking numbers generated from a *seed*. So, if you have a seed, generating pseudo-random numbers from it is very much a pure operation. This means that in JAX, we will always need to give every function that samples something (e.g., in initializing parameters) a seed to draw randomness from. Usually you'd nicely thread these through your program, but to keep it short we'll just hardcode them here. (Later you'll see us using them a bit more properly.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRVqQ2O50miq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hid_dim = 17\n",
        "lm = PytreeLSTMLM(\n",
        "    PytreeLSTMCell(\n",
        "        jax.random.uniform(jax.random.PRNGKey(1234), (4*hid_dim, hid_dim)),\n",
        "        jax.random.uniform(jax.random.PRNGKey(4321), (4*hid_dim, hid_dim)),\n",
        "        jnp.zeros((4*hid_dim,)),\n",
        "    ),\n",
        "    jax.random.uniform(jax.random.PRNGKey(123), (vocab_size, hid_dim)),\n",
        "    jnp.zeros((hid_dim,)),\n",
        ")\n",
        "\n",
        "# We are using a different LR, i.e., a new optimizer/combiner here\n",
        "update_combiner_01 = lambda p, g: update_combiner(p, g, lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnMhzslz0mEc",
        "colab_type": "text"
      },
      "source": [
        "To make sure JAX takes note of us constructing $(h_0, c_0)$ from $c_0$, which we deem a parameter, that too has to happen inside the “pure” function that we want to call `jax.grad` on, so we end up with this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtE2p2q5v4xO",
        "colab_type": "code",
        "outputId": "c036bf55-7153-4032-920f-f2910cb3fcb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "def pure_loss_fn(lm, seq, hc):\n",
        "    if hc is None:\n",
        "        hc = lm.hc_0\n",
        "    loss, _ = lm.forward(seq, hc)\n",
        "    return loss\n",
        "\n",
        "grad_fn = jax.grad(pure_loss_fn)\n",
        "\n",
        "print(\"Sample before:\", lm.greedy_argmax(lm.hc_0))\n",
        "\n",
        "bptt_length = 3\n",
        "for epoch in range(101):\n",
        "    totalloss = 0.\n",
        "    hc = None\n",
        "    for start in range(0, len(training_data), bptt_length):\n",
        "        batch = training_data[start:start+bptt_length]\n",
        "        loss, new_hc = lm.forward(batch, hc if hc else lm.hc_0)\n",
        "        if epoch % 50 == 0:\n",
        "            totalloss += loss.item()\n",
        "        grad_lm = grad_fn(lm, batch, hc)\n",
        "        lm = jax.tree_multimap(update_combiner_01, lm, grad_lm)\n",
        "        hc = new_hc\n",
        "    if totalloss:\n",
        "        print(\"Loss:\", totalloss)\n",
        "\n",
        "print(\"Sample after:\", lm.greedy_argmax(lm.hc_0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample before: [0, 25, 25, 25, 25, 25]\n",
            "Loss: 26.58103370666504\n",
            "Loss: 4.404336929321289\n",
            "Loss: 2.6979217529296875\n",
            "Sample after: [4, 8, 15, 16, 23, 42]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MofQutSSqfOW",
        "colab_type": "text"
      },
      "source": [
        "Well, it works! But... for one, writing the handlers was rather nasty. We could imagine a `nn.Module`-like class like in PyTorch that automatically builds these handlers from its parameters and submodules, if we give it some method to register and keep track of them—hold that thought for later!—this would allow us to write code that was a bit closer to PyTorch.\n",
        "\n",
        "However, there's also another annoyance with this code: we had to initialize all our parameters outside of the classes, far away from where we use them. That's really undesirable! Imagine we want to exchange our LSTM cell for a GRU cell. We would have to look up how exactly the GRU cell is implemented and what parameters of which shapes it uses, and then rewrite our initialization block to cater to that.\n",
        "\n",
        "In contrast, what we are going to work our way towards is a framework that allows us to specify initializations for parameters inside the modules they are used in themselves, and, more impressively yet, turn stateful looking interactions with these registered parameters into *pure* functions that JAX can easily operate on!\n",
        "\n",
        "\n",
        "## 4. Fancy objects manage parameters\n",
        "\n",
        "To see the basic idea in action, consider the good old `w * x` example:\n",
        "[![Purifying an impure function](https://sjmielke.com/tmp/purification_small.png)](https://sjmielke.com/tmp/purification_big.png)\n",
        "\n",
        "On the left is the class we'd like to write PyTorch-style, and on the right we have a class that houses a pure function like we've been writing manually before.\n",
        "\n",
        "The key insight is that we can very easily generate that “purified” function by controlling the context—specifically by setting `self.w` to reference the tensor `w` that was given as input, thus making sure that the formerly purity-violating access now only accesses inputs of the function, making the entire thing pure. What's important is that we don't need to know what `f` is doing with `x` or `w` when we write `purified_f`: all the logic of `f` is still happening, just now with a controlled context.\n",
        "\n",
        "Let's implement a “purifying” regressor that does exactly this. We have two parameters, `w` and `b`, so we're going back to the parameter dictionary approach to keep them together. We will also wrap the problematic access (the thick arrow) in a method call `get_param` instead of referencing `self.w` (or, really `self.params`) immediately. That (for now) will just make sure we are actually running in “pure mode” and throw an error otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkdyaApH56fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PurifyingRegressor():\n",
        "\n",
        "    # This is the function the user wants to write.\n",
        "    def impure_user_fn(self, x):\n",
        "        w = self.get_param('w')\n",
        "        b = self.get_param('b')\n",
        "        return w * x + b\n",
        "\n",
        "    # This is the wrapping/mode logic.\n",
        "    params = {}\n",
        "    is_running_pure = False\n",
        "\n",
        "    def get_param(self, name):\n",
        "        if not self.is_running_pure:\n",
        "            raise Exception(\"We can only call this when wrapped!\")\n",
        "        else:\n",
        "            return self.params[name]\n",
        "\n",
        "    # This is the function that we want JAX to use!\n",
        "    # Note how it defers all calculation and logic to\n",
        "    # the user-written and just provides *context*!\n",
        "    def pure_wrapped_user_fn(self, params, *args):\n",
        "        self.is_running_pure = True\n",
        "        self.params = params\n",
        "        result = self.impure_user_fn(*args)\n",
        "        self.is_running_pure = False\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpLR0b127BDy",
        "colab_type": "text"
      },
      "source": [
        "Convince yourself that it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SW4ZZE2fUsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pure_predict_fn = PurifyingRegressor().pure_wrapped_user_fn\n",
        "\n",
        "# Show that it's equal to this manually purified function:\n",
        "manually_pure_predict_fn = lambda params, x: params['w'] * x + params['b']\n",
        "\n",
        "args = ({'w': 13., 'b': 0.}, 42.)\n",
        "\n",
        "assert pure_predict_fn(*args) == manually_pure_predict_fn(*args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rnLXJjufVYz",
        "colab_type": "text"
      },
      "source": [
        "If that all makes sense to you, let's move on to introducing a second mode: an *initialization* mode! The reason is that remembering all parameters outside of the class and the code like we did above is really annoying. It would be much nicer if we could give an initialization for a parameter as we *use* it (this will also allow us to do some *shape inference*, e.g., only specify the output dimensionality of a linear layer and infer the input dimension from the incoming data). \n",
        "\n",
        "To do just that, we will extend `get_param` to also take an `initializer` argument (and extract all this magic that we're writing into a simple `nn.module`-like class the others will be able to inherit from):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cx8zT0ZL-_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlatFancyNNModule():\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.params = None\n",
        "        self.are_we_pure = False\n",
        "        self.are_we_initializing = False\n",
        "\n",
        "    # This is where, depending on mode, we'll give the user-written\n",
        "    # impure function different things!\n",
        "    def get_param(self, name, shape, initializer):\n",
        "        if self.are_we_initializing:\n",
        "            if name not in self.params:\n",
        "                self.params[name] = initializer(shape)\n",
        "            return self.params[name]\n",
        "        elif self.are_we_pure:\n",
        "            return self.params[name]\n",
        "        else:\n",
        "            raise Exception(\"Can't access parameters outside of context!\")\n",
        "\n",
        "    # This function calls the initializers to give us an initial params dict.    \n",
        "    def initial_params(self, method, *args):\n",
        "        self.are_we_initializing = True\n",
        "        self.params = {}\n",
        "        method(*args)\n",
        "        self.are_we_initializing = False\n",
        "        return self.params\n",
        "\n",
        "    # This function returns a pure version of the function.\n",
        "    def purify_method(self, method):\n",
        "        def pure_method(params, *args):\n",
        "            self.are_we_pure = True\n",
        "            self.params = params\n",
        "            result = method(*args)\n",
        "            self.are_we_pure = False\n",
        "            self.params = None\n",
        "            return result\n",
        "        return pure_method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwX5EVkrjQmT",
        "colab_type": "text"
      },
      "source": [
        "So, to recap, there are now two “modes” that an impure function we wrote can be called in:\n",
        "1. initialization mode, where accessing a parameter calls the initializer, the result is a `params` dict with all these initial values\n",
        "2. pure running mode, where we simulate purity by making sure the “stateful” class member `params` is actually the pure argument that is fed in.\n",
        "\n",
        "Let's test it on our simple example! An implementation of a user-written module will look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtEfu3XETkZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FancyLinearRegressor(FlatFancyNNModule):\n",
        "    def __init__(self, name=\"linreg\"):\n",
        "        super().__init__(name=name)\n",
        "    \n",
        "    def predict(self, x):\n",
        "        # Our \"initializers\" are a bit simple here.\n",
        "        w = self.get_param('w', (0,), lambda _shape: 13.)\n",
        "        b = self.get_param('b', (0,), lambda _shape: 0.)\n",
        "        return w * x + b\n",
        "\n",
        "    def rms(self, xs, ys):\n",
        "        # Same as before.\n",
        "        w = self.get_param('w', (0,), lambda _shape: 13.)\n",
        "        b = self.get_param('b', (0,), lambda _shape: 0.)\n",
        "        return jnp.sqrt(jnp.sum(jnp.square(w * xs + b - ys)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwynSVzzCsrx",
        "colab_type": "text"
      },
      "source": [
        "And this is how we would transform it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geI2TxgLCuNs",
        "colab_type": "code",
        "outputId": "be8c3ec2-e22f-4c16-f3f4-85998121d16a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "my_regressor = FancyLinearRegressor()\n",
        "\n",
        "sample_input = 999.  # a placeholder, necessary so the method can be run completely\n",
        "params = my_regressor.initial_params(my_regressor.predict, sample_input)\n",
        "\n",
        "print(\"Params:\", params)\n",
        "\n",
        "pure_predict_fn = my_regressor.purify_method(my_regressor.predict)\n",
        "\n",
        "pure_predict_fn(params, 42.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params: {'w': 13.0, 'b': 0.0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "546.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BpjWhmEDDc4",
        "colab_type": "text"
      },
      "source": [
        "Nice! And of course, we can also train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1lsdZOo-3Zp",
        "colab_type": "code",
        "outputId": "a95bd39b-dbe5-4ca2-9b4a-01d8263cdd3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "pure_rms_fn = my_regressor.purify_method(my_regressor.rms)\n",
        "grad_rms_fn = jax.grad(pure_rms_fn)\n",
        "\n",
        "params = my_regressor.initial_params(my_regressor.predict, sample_input)\n",
        "for _ in range(15):\n",
        "    print(pure_rms_fn(params, xs, ys))\n",
        "    grads = grad_rms_fn(params, xs, ys)\n",
        "    params = jax.tree_multimap(update_combiner, params, grads)\n",
        "\n",
        "pure_predict_fn(params, 42.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.0\n",
            "42.47003\n",
            "38.940002\n",
            "35.410034\n",
            "31.880066\n",
            "28.350098\n",
            "24.820068\n",
            "21.2901\n",
            "17.760132\n",
            "14.230164\n",
            "10.700165\n",
            "7.170166\n",
            "3.6401978\n",
            "0.110198975\n",
            "3.4197998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(500.1102, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVirCknWlV_g",
        "colab_type": "text"
      },
      "source": [
        "And it works! So now I hope you really want to move on and see our LSTM-LM implemented—but if you were to do that right now, you'd notice an issue: our implementation only works on parameters that are directly requested inside the module itself! It doesn't support submodules—but we definitely want those for our LM, which will both own some parameters and own a `LSTMCell` class that owns its own. So, to make sure that submodules of a given object that are also `FancyNNModule` descendants follow suit, we'll update our implementation also keep a list of `submodules` around and recurse through all those on all mode changes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hcjN3S0jN70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FancyNNModule():\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.params = None\n",
        "        self.are_we_pure = False\n",
        "        self.are_we_initializing = False\n",
        "        # Implementations should register their submodules here!\n",
        "        self.submodules = {}\n",
        "\n",
        "    # This is where, depending on mode, we'll give the user-written\n",
        "    # impure function different things!\n",
        "    def get_param(self, name, shape, initializer):\n",
        "        if self.are_we_initializing:\n",
        "            if name not in self.params:\n",
        "                self.params[name] = initializer(shape)\n",
        "            return self.params[name]\n",
        "        elif self.are_we_pure:\n",
        "            return self.params[name]\n",
        "        else:\n",
        "            raise Exception(\"Can't access parameters outside of context!\")\n",
        "\n",
        "    def set_are_we_pure(self, ispure):\n",
        "        if ispure: self.params = {}\n",
        "        self.are_we_pure = ispure\n",
        "        for sm in self.submodules.values():\n",
        "            sm.set_are_we_pure(ispure)\n",
        "        if not ispure: self.params = None\n",
        "\n",
        "    def set_are_we_initializing(self, isinit):\n",
        "        if isinit:\n",
        "            self.params = {}\n",
        "        self.are_we_initializing = isinit\n",
        "        for sm in self.submodules.values():\n",
        "            sm.set_are_we_initializing(isinit)\n",
        "\n",
        "    # This method gathers params from this module and all submodules.\n",
        "    def gather_params(self):\n",
        "        params = self.params\n",
        "        for sm_name, sm in self.submodules.items():\n",
        "            for name, value in sm.gather_params().items():\n",
        "                params[sm_name + \"/\" + name] = value\n",
        "        return params\n",
        "\n",
        "    # This method spreads out params into self and all submodules.\n",
        "    def spread_params(self, params):\n",
        "        for name, value in params.items():\n",
        "            path = name.split(\"/\")\n",
        "            if len(path) == 1:\n",
        "                self.params[name] = value\n",
        "            else:\n",
        "                self.submodules[path[0]].spread_params({\"/\".join(path[1:]): value})\n",
        "\n",
        "    # This function calls the initializers to give us an initial params dict.    \n",
        "    def initial_params(self, method, *args):\n",
        "        self.set_are_we_initializing(True)\n",
        "        method(*args)\n",
        "        self.set_are_we_initializing(False)\n",
        "        params = self.gather_params()\n",
        "        return params\n",
        "\n",
        "    # This function returns a pure version of the function.\n",
        "    def purify_method(self, method):\n",
        "        def pure_method(params, *args):\n",
        "            self.set_are_we_pure(True)\n",
        "            self.spread_params(params)\n",
        "            result = method(*args)\n",
        "            self.set_are_we_pure(False)\n",
        "            return result\n",
        "        return pure_method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWiqOBR4A5M2",
        "colab_type": "text"
      },
      "source": [
        "Note that this is still a very naive implementation that will not work for general cases, but it will serve to illustrate the *rough* idea—a real implementation would properly manage a *frame* *stack*.\n",
        "\n",
        "But, foregoing that, let's finally implement the LSTM-LM for the third time (this time, we'll use `jax.random.split` for threading randomness, yes through an ugly global variable... but shhhh...):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbsUQC5gEOPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_key = jax.random.PRNGKey(0)\n",
        "\n",
        "def unif_initializer(shape):\n",
        "    global random_key\n",
        "    sample_key, random_key = jax.random.split(random_key)\n",
        "    return jax.random.uniform(sample_key, shape)\n",
        "\n",
        "class FancyLSTMCell(FancyNNModule):\n",
        "    def __init__(self, hid_dim, name=\"lstmcell\"):\n",
        "        super().__init__(name=name)\n",
        "        self.hid_dim = hid_dim\n",
        "\n",
        "    def __call__(self, inputs, h, c):\n",
        "        weight_ih = self.get_param(\"weight_ih\",\n",
        "                (4*self.hid_dim, self.hid_dim), unif_initializer)\n",
        "        weight_hh = self.get_param(\"weight_hh\",\n",
        "                (4*self.hid_dim, self.hid_dim), unif_initializer)\n",
        "        bias = self.get_param(\"bias\",\n",
        "                (4*self.hid_dim,), lambda shape: jnp.zeros(shape))\n",
        "\n",
        "        ifgo = weight_ih @ inputs + weight_hh @ h + bias\n",
        "        i, f, g, o = jnp.split(ifgo, indices_or_sections=4, axis=-1)\n",
        "        i = jax.nn.sigmoid(i)\n",
        "        f = jax.nn.sigmoid(f)\n",
        "        g = jnp.tanh(g)\n",
        "        o = jax.nn.sigmoid(o)\n",
        "        new_c = f * c + i * g\n",
        "        new_h = o * jnp.tanh(new_c)\n",
        "        return (new_h, new_c)\n",
        "\n",
        "class FancyLSTMLM(FancyNNModule):\n",
        "    def __init__(self, vocab_size, dim, name=\"lstmlm\"):\n",
        "        super().__init__(name=name)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        # Now create a submodule and register it!\n",
        "        self.cell = FancyLSTMCell(dim)\n",
        "        self.submodules[self.cell.name] = self.cell\n",
        "    \n",
        "    @property\n",
        "    def hc_0(self):\n",
        "        _c_0 = self.get_param(\"c_0\",\n",
        "                (self.dim,), lambda shape: jnp.zeros(shape))\n",
        "        return (jnp.tanh(_c_0), _c_0)\n",
        "\n",
        "    def forward(self, seq, hc):\n",
        "        loss = 0.\n",
        "        embeddings = self.get_param(\"embeddings\",\n",
        "                (self.vocab_size, self.dim), unif_initializer)\n",
        "        for idx in seq:\n",
        "            loss -= jax.nn.log_softmax(embeddings @ hc[0])[idx]\n",
        "            hc = self.cell(embeddings[idx,:], *hc)\n",
        "        return loss, hc\n",
        "\n",
        "    def greedy_argmax(self, hc, length=6):\n",
        "        idxs = []\n",
        "        embeddings = self.get_param(\"embeddings\",\n",
        "                (self.vocab_size, self.dim), unif_initializer)\n",
        "        for i in range(length):\n",
        "            idx = jnp.argmax(embeddings @ hc[0])\n",
        "            idxs.append(int(idx))\n",
        "            hc = self.cell(embeddings[idx,:], *hc)\n",
        "        return idxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv1vWH2JFyf_",
        "colab_type": "text"
      },
      "source": [
        "Again, we have to make sure we “catch” the $(h,c)_0$ construction in our transformation process, so we define three pure functions and start training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSqufqMMFIcc",
        "colab_type": "code",
        "outputId": "351edec5-fcdf-4098-d110-5d2e903d6b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "lm = FancyLSTMLM(vocab_size, 17)\n",
        "\n",
        "# Since jitting and our mutable-state-galore \"tracer\" don't play nice, only\n",
        "# call jit on the outermost level: the actually pure function!\n",
        "pure_sample_fn = lm.purify_method(\n",
        "    lambda: lm.greedy_argmax(lm.hc_0))\n",
        "pure_forward_fn = jax.jit(lm.purify_method(\n",
        "    lambda seq, hc: lm.forward(seq, hc if hc else lm.hc_0)))\n",
        "grad_loss_fn = jax.jit(jax.grad(lm.purify_method(\n",
        "    lambda seq, hc: lm.forward(seq, hc if hc else lm.hc_0)[0])))\n",
        "\n",
        "params = lm.initial_params(lambda: lm.forward(jnp.array([0]), lm.hc_0))\n",
        "print(\"All parameters, recursively found:\", list(params.keys()))\n",
        "\n",
        "print(\"Sample before:\", pure_sample_fn(params))\n",
        "\n",
        "bptt_length = 3\n",
        "for epoch in range(101):\n",
        "    totalloss = 0.\n",
        "    hc = None\n",
        "    for start in range(0, len(training_data), bptt_length):\n",
        "        batch = training_data[start:start+bptt_length]\n",
        "        loss, new_hc = pure_forward_fn(params, batch, hc)\n",
        "        if epoch % 50 == 0:\n",
        "            totalloss += loss.item()\n",
        "        grads = grad_loss_fn(params, batch, hc)\n",
        "        params = jax.tree_multimap(update_combiner_01, params, grads)\n",
        "        hc = new_hc\n",
        "    if totalloss:\n",
        "        print(\"Loss:\", totalloss)\n",
        "\n",
        "print(\"Sample after:\", pure_sample_fn(params))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All parameters, recursively found: ['c_0', 'embeddings', 'lstmcell/weight_ih', 'lstmcell/weight_hh', 'lstmcell/bias']\n",
            "Sample before: [0, 20, 20, 20, 20, 20]\n",
            "Loss: 24.401604652404785\n",
            "Loss: 3.097736358642578\n",
            "Loss: 1.7444753646850586\n",
            "Sample after: [4, 8, 15, 16, 23, 42]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BzaOoNvSpEc",
        "colab_type": "text"
      },
      "source": [
        "## 5. We could've used `haiku.transform` all along\n",
        "\n",
        "Here's the plot twist: what we've done is basically implement a poor version of `haiku`'s `transform()`! The function `haiku.transform(impure_fn)` return both an `init_fn` and an `apply_fn`. The `init_fn` corresponds to our `initial_params()` method, the `apply_fn` is the “purified” version of the `impure_fn` we provide.\n",
        "\n",
        "So, without further ado, let's do one last LSTM implementation, using `haiku.transform()` and a mixture of self-allocated parameters and `haiku` layers (which, again, don't take as a particularly pretty canonical example):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cZznGs5UGAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import haiku as hk\n",
        "\n",
        "class HaikuLSTMCell(hk.Module):\n",
        "    def __init__(self, in_dim, out_dim, name=None):\n",
        "        super().__init__(name=name or \"lstmcell\")\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "    def __call__(self, inputs, h, c):\n",
        "        weight_ih = hk.get_parameter(\"weight_ih\",\n",
        "                (4*self.out_dim, self.in_dim),\n",
        "                init=hk.initializers.UniformScaling())\n",
        "        weight_hh = hk.get_parameter(\"weight_hh\",\n",
        "                (4*self.out_dim, self.out_dim),\n",
        "                init=hk.initializers.UniformScaling())\n",
        "        bias = hk.get_parameter(\"bias\",\n",
        "                (4*self.out_dim,),\n",
        "                init=hk.initializers.Constant(0.0))\n",
        "\n",
        "        ifgo = weight_ih @ inputs + weight_hh @ h + bias\n",
        "        i, f, g, o = jnp.split(ifgo, indices_or_sections=4, axis=-1)\n",
        "        i = jax.nn.sigmoid(i)\n",
        "        f = jax.nn.sigmoid(f)\n",
        "        g = jnp.tanh(g)\n",
        "        o = jax.nn.sigmoid(o)\n",
        "        new_c = f * c + i * g\n",
        "        new_h = o * jnp.tanh(new_c)\n",
        "        return (new_h, new_c)\n",
        "\n",
        "class HaikuLSTMLM(hk.Module):\n",
        "    def __init__(self, vocab_size, dim, name=None):\n",
        "        super().__init__(name=name or \"lstmlm\")\n",
        "        _c0 = hk.get_parameter(\"c_0\",\n",
        "                (dim,),\n",
        "                init=hk.initializers.TruncatedNormal(stddev=0.1))\n",
        "        self.hc_0 = (jnp.tanh(_c0), _c0)\n",
        "        self.embeddings = hk.Embed(vocab_size, dim)\n",
        "        self.cell = HaikuLSTMCell(dim, dim)\n",
        "\n",
        "    # @jax.jit\n",
        "    def forward(self, seq, hc):\n",
        "        loss = 0.\n",
        "        for idx in seq:\n",
        "            loss -= jax.nn.log_softmax(self.embeddings.embeddings @ hc[0])[idx]\n",
        "            hc = self.cell(self.embeddings(idx), *hc)\n",
        "        return loss, hc\n",
        "\n",
        "    def greedy_argmax(self, hc, length=6):\n",
        "        idxs = []\n",
        "        for i in range(length):\n",
        "            idx = jnp.argmax(self.embeddings.embeddings @ hc[0])\n",
        "            idxs.append(int(idx))\n",
        "            hc = self.cell(self.embeddings(idx), *hc)\n",
        "        return idxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSQkPPpBkCDE",
        "colab_type": "text"
      },
      "source": [
        "Note that because `Haiku` mandates that all `__init__` calls of `haiku.Module`s happen inside a `transform`, we just go ahead and also allocate our parameters and `haiku` modules right there—as long as that `__init__` is part of the impure function you transform in the end, that's no problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u4hewackIGV",
        "colab_type": "code",
        "outputId": "04f45d7e-95d4-452d-f9cb-f892f5b42126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "def impure_sample_fn():\n",
        "    lm = HaikuLSTMLM(vocab_size, 17)\n",
        "    return lm.greedy_argmax(lm.hc_0)\n",
        "\n",
        "def impure_forward_fn(seq, hc):\n",
        "    lm = HaikuLSTMLM(vocab_size, 17)\n",
        "    return lm.forward(seq, hc if hc else lm.hc_0)\n",
        "\n",
        "_, pure_sample_fn = hk.transform(impure_sample_fn)\n",
        "init_fn, nojit_pure_forward_fn = hk.transform(impure_forward_fn)\n",
        "_, nojit_pure_loss_fn = hk.transform(lambda *args: impure_forward_fn(*args)[0])\n",
        "\n",
        "pure_forward_fn = jax.jit(nojit_pure_forward_fn)\n",
        "pure_loss_fn = jax.jit(nojit_pure_loss_fn)\n",
        "grad_loss_fn = jax.jit(jax.grad(nojit_pure_loss_fn))\n",
        "\n",
        "rng = jax.random.PRNGKey(0)  # Haiku actually manages the random number generator :)\n",
        "params = init_fn(rng, jnp.array([0]), None)\n",
        "\n",
        "from haiku._src.data_structures import frozendict\n",
        "def print_params(params):\n",
        "    return [\n",
        "            (name, print_params(value))\n",
        "            if isinstance(value, frozendict)\n",
        "            else name\n",
        "            for name, value in params.items()\n",
        "    ]\n",
        "print(\"All parameters, recursively found:\", print_params(params))\n",
        "\n",
        "print(\"Sample before:\", pure_sample_fn(params))\n",
        "\n",
        "bptt_length = 3\n",
        "for epoch in range(101):\n",
        "    totalloss = 0.\n",
        "    hc = None\n",
        "    for start in range(0, len(training_data), bptt_length):\n",
        "        batch = training_data[start:start+bptt_length]\n",
        "        loss, new_hc = pure_forward_fn(params, batch, hc)\n",
        "        if epoch % 50 == 0:\n",
        "            totalloss += loss.item()\n",
        "        grads = grad_loss_fn(params, batch, hc)\n",
        "        params = jax.tree_multimap(update_combiner_01, params, grads)\n",
        "        hc = new_hc\n",
        "    if totalloss:\n",
        "        print(\"Loss:\", totalloss)\n",
        "\n",
        "print(\"Sample after:\", pure_sample_fn(params))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All parameters, recursively found: [('lstmlm', ['c_0']), ('lstmlm/~/embed', ['embeddings']), ('lstmlm/~/lstmcell', ['bias', 'weight_hh', 'weight_ih'])]\n",
            "Sample before: [23, 34, 7, 34, 7, 34]\n",
            "Loss: 23.327383041381836\n",
            "Loss: 1.4916257858276367\n",
            "Loss: 1.1180200576782227\n",
            "Sample after: [4, 8, 15, 16, 23, 42]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JvyIJawS8uX",
        "colab_type": "text"
      },
      "source": [
        "So, with all this said and done, I hope you feel a little more comfortable with JAX and are ready to start coding your own stuff!\n",
        "As a final bonus question: how do you think JAX does it? The answer might surprise you... (and if you actually watched Skye's talk, you know it: JAX builds up a computation graph internally with placeholder nodes!)\n",
        "\n",
        "There's plenty more to explore and figure out there still, but with this knowledge you should have a rough idea of what you're looking for in frameworks like `flax`, `trax`, and `haiku`, or, if you don't mind writing pure code all throughout, how to do just that! :) Or, if you have the time and energy, try and implement a “mini-framework” yourself, maybe based on the ideas above—who knows, maybe you'll find *just the right* abstraction? ;)\n",
        "\n",
        "Also, if you want to start using JAX for your code, make sure to read up on `jit()`, `vmap()`, `pmap()` and all the other things it offers! There's lots of cool stuff to cover at a future time...\n",
        "\n",
        "Thanks for reading! Feedback welcome on Twitter: @sjmielke.\n",
        "\n",
        "---\n",
        "\n",
        "*Thanks to Igor Babuschkin, Joost Bastings, Anton Belyy, Jason Eisner, Matthew Honnibal, Xiang Lorraine Li, Madison May, Pamela Shapiro, and Suzanna Sia for their feedback, comments, and suggestion on a draft of this post!*\n",
        "\n",
        "---\n",
        "\n",
        "Please feel free to cite this post using BibTeX like this:\n",
        "\n",
        "```\n",
        "@misc{\n",
        "\tMie2020From,\n",
        "\ttitle={From PyTorch to JAX: towards neural net frameworks that purify stateful code},\n",
        "\turl={https://sjmielke.com/jax-purify.htm},\n",
        "\tauthor={Sabrina J. Mielke},\n",
        "\tyear={2020},\n",
        "\tmonth={Mar}\n",
        "}\n",
        "```"
      ]
    }
  ]
}